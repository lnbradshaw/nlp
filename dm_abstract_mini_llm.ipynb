{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6ad4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import json\n",
    "import pandas as pd\n",
    "import arxiv\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3a0fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79529177",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv.Search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45936f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use arxiv API to get abstracts from hep-ph papers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m search \u001b[38;5;241m=\u001b[39m arxiv\u001b[38;5;241m.\u001b[39mSearch(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat:hep-ph\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                      max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30000\u001b[39m,\n\u001b[1;32m      5\u001b[0m                      sort_order\u001b[38;5;241m=\u001b[39marxiv\u001b[38;5;241m.\u001b[39mSortOrder\u001b[38;5;241m.\u001b[39mDescending)\n\u001b[0;32m----> 6\u001b[0m abs_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(result\u001b[38;5;241m.\u001b[39msummary) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m search\u001b[38;5;241m.\u001b[39mresults()]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(abs_list))\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use arxiv API to get abstracts from hep-ph papers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m search \u001b[38;5;241m=\u001b[39m arxiv\u001b[38;5;241m.\u001b[39mSearch(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat:hep-ph\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                      max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30000\u001b[39m,\n\u001b[1;32m      5\u001b[0m                      sort_order\u001b[38;5;241m=\u001b[39marxiv\u001b[38;5;241m.\u001b[39mSortOrder\u001b[38;5;241m.\u001b[39mDescending)\n\u001b[0;32m----> 6\u001b[0m abs_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(result\u001b[38;5;241m.\u001b[39msummary) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m search\u001b[38;5;241m.\u001b[39mresults()]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(abs_list))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/arxiv/arxiv.py:594\u001b[0m, in \u001b[0;36mClient.results\u001b[0;34m(self, search, offset)\u001b[0m\n\u001b[1;32m    589\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m results at offset \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    590\u001b[0m     page_size,\n\u001b[1;32m    591\u001b[0m     offset,\n\u001b[1;32m    592\u001b[0m ))\n\u001b[1;32m    593\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_url(search, offset, page_size)\n\u001b[0;32m--> 594\u001b[0m feed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_page:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# NOTE: this is an ugly fix for a known bug. The totalresults\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# value is set to 1 for results with zero entries. If that API\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;66;03m# bug is fixed, we can remove this conditional and always set\u001b[39;00m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;66;03m# `total_results = min(...)`.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feed\u001b[38;5;241m.\u001b[39mentries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/arxiv/arxiv.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m`self.num_retries` times.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Invoke the recursive helper with initial available retries.\u001b[39;00m\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_left\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_retries\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/arxiv/arxiv.py:681\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[0;34m(self, url, first_page, retries_left, last_err)\u001b[0m\n\u001b[1;32m    674\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(to_sleep)\n\u001b[1;32m    675\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page of results\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: url,\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_page\u001b[39m\u001b[38;5;124m'\u001b[39m: first_page,\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m'\u001b[39m: retry,\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_err\u001b[39m\u001b[38;5;124m'\u001b[39m: last_err\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;28;01mif\u001b[39;00m last_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    680\u001b[0m })\n\u001b[0;32m--> 681\u001b[0m feed \u001b[38;5;241m=\u001b[39m \u001b[43mfeedparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    683\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/feedparser/api.py:263\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, response_headers, resolve_relative_uris, sanitize_html)\u001b[0m\n\u001b[1;32m    261\u001b[0m source\u001b[38;5;241m.\u001b[39msetByteStream(io\u001b[38;5;241m.\u001b[39mBytesIO(data))\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[43msaxparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xml\u001b[38;5;241m.\u001b[39msax\u001b[38;5;241m.\u001b[39mSAXException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    265\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbozo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/xml/sax/expatreader.py:111\u001b[0m, in \u001b[0;36mExpatParser.parse\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cont_handler\u001b[38;5;241m.\u001b[39msetDocumentLocator(ExpatLocator(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mxmlreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIncrementalParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# bpo-30264: Close the source on error to not leak resources:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# xml.sax.parse() doesn't give access to the underlying parser\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# to the caller\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_source()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/xml/sax/xmlreader.py:125\u001b[0m, in \u001b[0;36mIncrementalParser.parse\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    123\u001b[0m buffer \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bufsize)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m buffer:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     buffer \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bufsize)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/xml/sax/expatreader.py:217\u001b[0m, in \u001b[0;36mExpatParser.feed\u001b[0;34m(self, data, isFinal)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cont_handler\u001b[38;5;241m.\u001b[39mstartDocument()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# The isFinal parameter is internal to the expat reader.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# If it is set to true, expat will check validity of the entire\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# document. When feeding chunks, they are not normally final -\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# except when invoked from close.\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misFinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m expat\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    219\u001b[0m     exc \u001b[38;5;241m=\u001b[39m SAXParseException(expat\u001b[38;5;241m.\u001b[39mErrorString(e\u001b[38;5;241m.\u001b[39mcode), e, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/Users/runner/miniforge3/conda-bld/python-split_1643750240456/work/Modules/pyexpat.c:272\u001b[0m, in \u001b[0;36mCharacterData\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/feedparser/parsers/strict.py:106\u001b[0m, in \u001b[0;36m_StrictFeedParser.characters\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcharacters\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/feedparser/mixin.py:386\u001b[0m, in \u001b[0;36m_FeedParserMixin.handle_data\u001b[0;34m(self, text, escape)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melementstack:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m escape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontentparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/xhtml+xml\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    387\u001b[0m     text \u001b[38;5;241m=\u001b[39m xml\u001b[38;5;241m.\u001b[39msax\u001b[38;5;241m.\u001b[39msaxutils\u001b[38;5;241m.\u001b[39mescape(text)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melementstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(text)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/feedparser/util.py:135\u001b[0m, in \u001b[0;36mFeedParserDict.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    :return: A :class:`FeedParserDict`.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use arxiv API to get abstracts from hep-ph papers\n",
    "\n",
    "search = arxiv.Search(query=\"cat:hep-ph\",\n",
    "                     max_results=30000,\n",
    "                     sort_order=arxiv.SortOrder.Descending)\n",
    "abs_list = [str(result.summary) for result in search.results()]\n",
    "\n",
    "print(len(abs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some preprocessing on this data with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by tokenizing the text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True,lower=True)\n",
    "tokenizer.fit_on_texts(abs_list)\n",
    "\n",
    "# convert text to sequences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(abs_list)[0]\n",
    "\n",
    "# prepare input and target sequences\n",
    "input_seq = []\n",
    "output_seq = []\n",
    "seq_length = 100\n",
    "for i in range(len(sequences)-seq_length):\n",
    "    input_seq.append(sequences[i:i+seq_length])\n",
    "    output_seq.append(sequences[i+seq_length])\n",
    "input_seq = np.array(input_seq)\n",
    "output_seq = np.array(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56243999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up everything needed to define the model architecture\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=10,verbose=1,min_delta=1e-4)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',patience=5,factor=0.1,verbose=1,min_delta=1e-3)\n",
    "epochs = 150\n",
    "batch_size = 16\n",
    "\n",
    "# define model architecture\n",
    "\n",
    "mini_llm = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 50, input_length=seq_length),\n",
    "    tf.keras.layers.Dense(500, activation='elu'),\n",
    "    #tf.keras.layers.Dense(500, activation='elu'),\n",
    "    tf.keras.layers.LSTM(2048, return_sequences=True, dropout=0.15, recurrent_dropout=0),\n",
    "    #tf.keras.layers.LSTM(2048, return_sequences=True, dropout=0.15, recurrent_dropout=0),\n",
    "    tf.keras.layers.LSTM(2048, dropout=0.15, recurrent_dropout=0),\n",
    "    #tf.keras.layers.Dense(500, activation='elu'),\n",
    "    tf.keras.layers.Dense(500, activation='elu'),\n",
    "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile model, print summary\n",
    "\n",
    "mini_llm.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                 metrics=['accuracy'])\n",
    "mini_llm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aa251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "mini_llm.fit(input_seq, output_seq, epochs=epochs, batch_size=batch_size,\n",
    "             verbose=1, callbacks=[reduce_lr,es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mini_llm.save('dm_abstract_mini_llm_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the model and generate some new text!\n",
    "\n",
    "def generate_text(seed, model, tokenizer, seq_length, num_char_to_gen=300):\n",
    "    generated_text = seed\n",
    "    \n",
    "    for _ in range(num_char_to_gen):\n",
    "        token_list = tokenizer.texts_to_sequences([generated_text])\n",
    "        token_list = tf.keras.preprocessing.sequence.pad_sequences(token_list, maxlen=seq_length, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted_token = np.argmax(predicted_probs, axis=-1)[0]\n",
    "        #print(predicted_token)\n",
    "        \n",
    "        #output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_token:\n",
    "                output_word = word\n",
    "                #print(output_word)\n",
    "                break\n",
    "        \n",
    "        generated_text += output_word\n",
    "        \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c04573",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = 'Dark matter '\n",
    "\n",
    "gen_text = generate_text(seed_text, mini_llm, tokenizer, seq_length, num_char_to_gen=500-len(seed_text))\n",
    "\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe36f0c",
   "metadata": {},
   "source": [
    "We roure mixing parameter\n",
    "scales linearly with the dark photon mass\n",
    "of order $10^{-20}$ ev. furthermore, the constraint on the mixing parameter\n",
    "scales linearly with the dark photon mass and so new significant constraints\n",
    "can be placed on the dark matter mass all the way up to $10^{-14}$ ev. future\n",
    "experiments measuring $g-2$ will probe even smaller gauge mixing parameters.ithe cork matter is a dark photon,\n",
    "the correction to the anomalous magnetic moment is larger than experimental\n",
    "uncertainties \n",
    "\n",
    "Dark matter is a dark photon,\n",
    "the correction to the anomalous magnetic moment is larger than experimental\n",
    "uncertainties for a mixing parameter of order $10^{-16}$ and a dark photon mass\n",
    "of order $10^{-20}$ ev. furthermore, the constraint on the mixing parameter\n",
    "scales linearly with the dark photon mass and so new significant constraints\n",
    "can be placed on the dark matter mass all the way up to $10^{-14}$ ev. future\n",
    "experiments measuring $g-2$ will probe even smaller gauge mixing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd566b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_llm_2 = tf.keras.models.load_model('dm_abstract_mini_llm_2.h5')\n",
    "\n",
    "seed_text = 'Dark '\n",
    "\n",
    "print(generate_text(seed_text, mini_llm_2, tokenizer, seq_length, num_char_to_gen=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63137a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
